{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Implementation from Scratch\n",
    "\n",
    "Now that you have some background on the *ideas* behind linear regression,\n",
    "we are ready to step through a hands-on implementation. \n",
    "In this section, and similar ones that follow, \n",
    "we are going to implement all parts of linear regression:\n",
    "the data pipeline, the model, the loss function, \n",
    "and the gradient descent optimizer, from scratch.\n",
    "Not surprisingly, today's deep learning frameworks\n",
    "can automate nearly all of this work, \n",
    "but if you never learn to implement things from scratch,\n",
    "then you may never truly understand how the model works.\n",
    "Moreover, when it comes time to customize models,\n",
    "defining our own layers, loss functions, etc.,\n",
    "knowing how things work under the hood will come in handy.\n",
    "Thus, we start off describing how to implement linear regression\n",
    "relyin only on the primitives in the NDArray and `autograd` packages.\n",
    "In the section immediately following, we will present the compact implementation, using all of Gluon's bells and whistles,\n",
    "but this is where we dive into the details.\n",
    "\n",
    "To start off, we import the packages required to run this section's experiments: we'll be using `matplotlib` for plotting, setting it to embed in the GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies before importing\n",
    "!pip install mxnet-cu100\n",
    "!pip install d2l\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from mxnet import autograd, nd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data Sets\n",
    "\n",
    "For this demonstration, we will construct a simple artificial dataset \n",
    "so that we can easily visualize the data \n",
    "and compare the true pattern to the learned parameters.\n",
    "We will set the number of examples in our training set to be 1000 \n",
    "and the number of features (or covariates) to 2. \n",
    "This our synthetic dataset will be an object \n",
    "$\\mathbf{X}\\in \\mathbb{R}^{1000 \\times 2}$.\n",
    "In this example, we will synthesize our data by sampling \n",
    "each data point $\\mathbf{x}_i$ from a Gaussian distribution.\n",
    "\n",
    "Moreover, to make sure that our algorithm works, \n",
    "we will assume that the linearity assumption holds\n",
    "with true underlying parameters $\\mathbf{w} = [2, -3.4]^\\top$ and $b = 4.2$.\n",
    "Thus our synthetic labels will be given according to the \n",
    "following linear model which includes a noise term $\\epsilon$ to account for\n",
    "measurement errors on the features and labels:\n",
    "\n",
    "$$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\mathbf\\epsilon$$\n",
    "\n",
    "Following standard assumptions, we choose a noise term $\\epsilon$ \n",
    "that obeys a normal distribution with mean of $0$,\n",
    "and in this example, we'll set the its standard deviation to $0.01$. \n",
    "The following code generates our synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = nd.array([2, -3.4])\n",
    "true_b = 4.2\n",
    "features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))\n",
    "labels = nd.dot(features, true_w) + true_b\n",
    "labels += nd.random.normal(scale=0.01, shape=labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each row in `features` consists of a 2-dimensional data point and that each row in `labels` consists of a 1-dimensional target value (a scalar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "features[0], labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By generating a scatter plot using the second `features[:, 1]` and `labels`, we can clearly observe the linear correlation between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "def use_svg_display():\n",
    "    # Display in vector graphics\n",
    "    display.set_matplotlib_formats('svg')\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    use_svg_display()\n",
    "    # Set the size of the graph to be plotted\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "set_figsize()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(features[:, 1].asnumpy(), labels.asnumpy(), 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotting function `plt` as well as the `use_svg_display` and `set_figsize` functions are defined in the `d2l` package. Now that you know how to make plots yourself, we will call `d2l.plt` directly for future plotting. To print the vector diagram and set its size, we only need to call `d2l.set_figsize()` before plotting, because `plt` is a global variable in the `d2l` package.\n",
    "\n",
    "\n",
    "## Reading Data\n",
    "\n",
    "Recall that training models, consists of making multiple passes over the dataset, grabbing one mini-batch of examples at a time and using them to update our model. Since this process is so fundamental to training machine learning algortihms, we need a utility for shuffling the data and accessing in mini-batches. \n",
    "\n",
    "In the following code, we define a `data_iter` function to demonstrate one possible implementation of this functionality. \n",
    "The function takes a batch size, a design matrix containing the features, \n",
    "and a vector of labels, yielding minibatches of size `batch_size`, \n",
    "each consisting of a tuple of features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "# This function has been saved in the d2l package for future use\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features.take(j), labels.take(j)\n",
    "        # The “take” function will then return the corresponding element based\n",
    "        # on the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, note that we want to use reasonably sized minibatches to take advantage of the GPU hardware, which excel at parallelizing operations. Because each example can be fed through our models in parallel and the gradient of the loss function for each example can also be taken in parallel, GPUs allow us to process hundreds of examples in scarcely more time than it might take to process just a single example. \n",
    "\n",
    "To build some intuition, let's read and print the first small batch of data examples. The shape of the features in each mini-batch tells us both the mini-batch size and the number of input features. Likewise, our mini-batch of labels will have a shape given by `bach_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be no surprise that as we run the iterator, \n",
    "we will obtain distinct minibatches each time \n",
    "until all the data has been exhausted (try this). \n",
    "While the iterator implemented above is good for didactic purposes,\n",
    "it is inefficient in ways that might get us in trouble on real problems.\n",
    "For example, it requires that we load all data in memory \n",
    "and that we perform a lot of random memory access. \n",
    "The built-in iterators implemented in Apache MXNet \n",
    "are considerably efficient and they can deal \n",
    "both with data stored on file and data fed via a data stream.\n",
    "\n",
    "## Initialize Model Parameters\n",
    "\n",
    "Before we can begin optimizing our model's parameters by gradient descent,\n",
    "we need to have some parameters in the first place. \n",
    "In the following code, we initialize weights by sampling\n",
    "random numbers from a normal distribution with mean 0 \n",
    "and a standard deviation of 0.01, setting the bias $b$ to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "w = nd.random.normal(scale=0.01, shape=(num_inputs, 1))\n",
    "b = nd.zeros(shape=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have initialized our parameters,\n",
    "our next task is to update them until they fit our data sufficiently well.\n",
    "Each update will require taking the gradient \n",
    "(a multi-dimensional derivative) \n",
    "of our loss function with respect to the parameters. \n",
    "Given this gradient, we will update each parameter \n",
    "in the direction that reduces the loss. \n",
    "\n",
    "Since nobody wants to compute gradients explicitly \n",
    "(this is tedious and error prone),\n",
    "we use automatic differentiation to compute the gradient. \n",
    "See section [\"Automatic Gradient\"](../chapter_prerequisite/autograd.md) \n",
    "for more details.\n",
    "Recall from the autograd chapter\n",
    "that in order for `autograd` to know \n",
    "that it should store a gradient for our parameters, \n",
    "we need to invoke the `attach_grad` function, \n",
    "allocating memory to store the gradients that we plan to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "w.attach_grad()\n",
    "b.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "Next, we must define our model, \n",
    "relating its inputs and parameters to its outputs. \n",
    "Recall that to calculate the output of the linear model, \n",
    "we simply take the matrix-vector dot product \n",
    "of the examples $\\mathbf{X}$ and the models weights $w$,\n",
    "and add the offset $b$ to each example.\n",
    "Note that below `nd.dot(X, w)` is a vector and `b` is a scalar.\n",
    "Recall that when we add a vector and a scalar, \n",
    "the scalar is added to each component of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "# This function has been saved in the d2l package for future use\n",
    "def linreg(X, w, b):\n",
    "    return nd.dot(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Loss Function\n",
    "\n",
    "Since updating our model requires taking the gradient of our loss function,\n",
    "we ought to define the loss function first.\n",
    "Here we will use the squared loss function \n",
    "as described in the previous section. \n",
    "In the implementation, we need to transform the true value `y` into the predicted value's shape `y_hat`. \n",
    "The result returned by the following function \n",
    "will also be the same as the `y_hat` shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "# This function has been saved in the d2l package for future use\n",
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Optimization Algorithm\n",
    "\n",
    "As we discussed in the previous section, \n",
    "linear regression has a closed-form solution. \n",
    "However, this isn't a book about linear regression,\n",
    "its a book about deep learning.\n",
    "Since none of the other models that this book introduces \n",
    "can be solved analytically, we will take this opportunity to introduce your first working example of stochastic gradient descent (SGD). \n",
    "\n",
    "\n",
    "At each step, using one batch randomly drawn from our dataset,\n",
    "we'll estimate the gradient of the loss with respect to our parameters. \n",
    "Then, we'll update our parameters a small amount \n",
    "in the direction that reduces the loss. \n",
    "Assuming that the gradient has already been calculated,\n",
    "each parameter (`param`) already has its gradient stored in `param.grad`.\n",
    "The following code applies the SGD update,\n",
    "given a set of parameters, a learning rate, and a batch size.\n",
    "The size of the update step is determined by the learning rate `lr`. \n",
    "Because our loss is calculated as a sum over the batch of examples,\n",
    "we normalize our step size by the batch size (`batch_size`),\n",
    "so that the magnitude of a typical step size \n",
    "doesn't depend heavily our choice of the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "# This function has been saved in the d2l package for future use\n",
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we have all of the parts in place, \n",
    "we are ready to implement the main training loop.\n",
    "It is crucial that you understand this code \n",
    "because you will see training loops that are nearly identical to this one\n",
    "over and over again throughout your career in deep learning.\n",
    "\n",
    "In each iteration, we will grab minibatches of models,\n",
    "first passing them through our model to obtain a set of predictions.\n",
    "After calculating the loss, we will call the `backward` function \n",
    "to backpropagate through the network, storing the gradients \n",
    "with respect to each parameter in its corresponding `.grad` attribute.\n",
    "Finally, we will call the optimization algorithm `sgd` \n",
    "to update the model parameters. \n",
    "Since we previously set the batch size `batch_size` to 10, \n",
    "the loss shape `l` for each small batch is (10, 1).\n",
    "\n",
    "In summary, we'll execute the following loop:\n",
    "\n",
    "* Initialize parameters $(\\mathbf{w}, b)$\n",
    "* Repeat until done\n",
    "    * Compute gradient $\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w},b)} \\frac{1}{\\mathcal{B}} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^i, y^i, \\mathbf{w}, b)$\n",
    "    * Update parameters $(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\eta \\mathbf{g}$\n",
    "\n",
    "In the code below, `l` is a vector of the losses \n",
    "for each example in the minibatch. \n",
    "Because `l` is not a scalar variable, \n",
    "running `l.backward()` adds together the elements in `l` \n",
    "to obtain the new variable and then calculates the gradient.\n",
    "\n",
    "In each epoch (a pass through the data), \n",
    "we will iterate through the entire dataset \n",
    "(using the `data_iter` function) once \n",
    "passing through every examples in the training dataset \n",
    "(assuming the number of examples is divisible by the batch size). \n",
    "The number of epochs `num_epochs` and the learning rate `lr` are both hyper-parameters, which we set here to $3$ and $0.03$, respectively. Unfortunately, setting hyper-parameters is tricky \n",
    "and requires some adjustment by trial and error. \n",
    "We elide these details for now but revise them\n",
    "later in the chapter on [\"Optimization Algorithms\"](../chapter_optimization/index.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.03  # Learning rate\n",
    "num_epochs = 3  # Number of iterations\n",
    "net = linreg  # Our fancy linear model\n",
    "loss = squared_loss  # 0.5 (y-y')^2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Assuming the number of examples can be divided by the batch size, all\n",
    "    # the examples in the training data set are used once in one epoch\n",
    "    # iteration. The features and tags of mini-batch examples are given by X\n",
    "    # and y respectively\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X, w, b), y)  # Minibatch loss in X and y\n",
    "        l.backward()  # Compute gradient on l with respect to [w,b]\n",
    "        sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, because we used synthetic data (that we synthesized ourselves!),\n",
    "we know preisely what the true parameters are. Thus, we can evaluate our success in training by comparing the true parameters with those that we learned through our training loop. Indeed they turn out to be very close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "print('Error in estimating w', true_w - w.reshape(true_w.shape))\n",
    "print('Error in estimating b', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we should not take it for granted \n",
    "that we are able to reover the parameters accurately. \n",
    "This only happens for a special category problems: \n",
    "strongly convex optimization problems with 'enough' data to ensure \n",
    "that the noisy samples allow us to recover the underlying dependency. \n",
    "In most cases this is *not* the case. \n",
    "In fact, the parameters of a deep network are rarely the same (or even close) between two different runs, unless all conditions are identical, \n",
    "including the order in which the data is traversed. \n",
    "However, in machine learning we are typically less concerned \n",
    "with recovering true underlying parameters,\n",
    "and more concerned with parameters that lead to accurate prediction.\n",
    "Fortunately, even on difficult optimization problems,\n",
    "that stochastic gradient descent can often lead to remarkably good solutions, \n",
    "due in part to the fact that for the models we will be working with,\n",
    "there exist many sets of parameters that work well.\n",
    "\n",
    "## Summary\n",
    "\n",
    "We saw how a deep network can be implemented \n",
    "and optimized from scratch, using just NDArray and `autograd`, \n",
    "without any need for defining layers, fancy optimizers, etc. \n",
    "This only scratches the surface of what is possible. \n",
    "In the following sections, we will describe additional models \n",
    "based on the concepts that we have just introduced \n",
    "and learn how to implement them more concisely.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. What would happen if we were to initialize the weights $\\mathbf{w} = 0$. Would the algorithm still work?\n",
    "1. Assume that you're [Georg Simon Ohm](https://en.wikipedia.org/wiki/Georg_Ohm) trying to come up with a model between voltage and current. Can you use `autograd` to learn the parameters of your model.\n",
    "1. Can you use [Planck's Law](https://en.wikipedia.org/wiki/Planck%27s_law) to determine the temperature of an object using spectral energy density.\n",
    "1. What are the problems you might encounter if you wanted to extend `autograd` to second derivatives? How would you fix them?\n",
    "1.  Why is the `reshape` function needed in the `squared_loss` function?\n",
    "1. Experiment using different learning rates to find out how fast the loss function value drops.\n",
    "1. If the number of examples cannot be divided by the batch size, what happens to the `data_iter` function's behavior?\n",
    "\n",
    "## Scan the QR Code to [Discuss](https://discuss.mxnet.io/t/2332)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/d2l-ai/notebooks/master/img/qr_linear-regression-scratch.png\" alt=\"\" width=75 height=75/>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}