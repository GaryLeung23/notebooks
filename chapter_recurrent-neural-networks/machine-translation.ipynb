{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation and Data Sets\n",
    "\n",
    "Machine translation (MT) refers to the automatic translation of a segment of text\n",
    "from one language to another. Solving this problem with neural networks is often\n",
    "called neural machine translation (NMT). Compared to the language model we discussed before, a major difference for MT is that the output is a sequence of words instead of a single words. The length of the output sequence could be different to the source sequence length. In the rest of this section, we will demonstrate how to pre-process a MT dataset and transform it into a set of data batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies before importing\n",
    "!pip install mxnet-cu100\n",
    "!pip install d2l\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import collections\n",
    "import d2l\n",
    "import zipfile\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import utils as gutils, data as gdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Pre-process Data\n",
    "\n",
    "We first download a dataset that contains a set of English sentences with the corresponding French translations. As can be seen that each line contains a English sentence with its French translation, which are separated by a `TAB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "fname = gutils.download('http://www.manythings.org/anki/fra-eng.zip')\n",
    "with zipfile.ZipFile(fname, 'r') as f:\n",
    "    raw_text = f.read('fra.txt').decode(\"utf-8\")\n",
    "print(raw_text[0:95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words and punctuation marks should be separated by spaces. But this dataset has a few exceptions. We fix them by adding necessary spaces before punctuation marks, replacing non-breaking space with space. In addition, we convert all chars into lower cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_raw(text):\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "    out = ''\n",
    "    for i, char in enumerate(text.lower()):\n",
    "        if char in (',', '!', '.') and i > 0 and text[i-1] != ' ':\n",
    "            out += ' '\n",
    "        out += char\n",
    "    return out\n",
    "\n",
    "text = preprocess_raw(raw_text)\n",
    "print(text[0:95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "A word or a punctuation mark is treated as a token, then a sentence is a list of tokens. We convert the text data into a set of source (English) sentences, a list of list of tokens, and a set of target (French) sentences. To simplify the later model training, we only sample the first `num_examples` sentences pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "num_examples = 50000\n",
    "source, target = [], []\n",
    "for i, line in enumerate(text.split('\\n')):\n",
    "    if i > num_examples:\n",
    "        break\n",
    "    parts = line.split('\\t')\n",
    "    if len(parts) == 2:\n",
    "        source.append(parts[0].split(' '))\n",
    "        target.append(parts[1].split(' '))\n",
    "\n",
    "source[0:3], target[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the histogram of the number of tokens per sentence the following figure. As can be seen that a sentence in average contains 5 tokens, and most of them have less than 10 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "d2l.plt.hist([[len(l) for l in source], [len(l) for l in target]],\n",
    "             label=['source', 'target'])\n",
    "d2l.plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Now build a vocabulary for the source sentences and print its vocabulary sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab(tokens):\n",
    "    tokens = [token for line in tokens for token in line]\n",
    "    return d2l.Vocab(tokens, min_freq=3, use_special_tokens=True)\n",
    "\n",
    "src_vocab = build_vocab(source)\n",
    "len(src_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Since sentences have variable lengths, we define a `pad` function to trim or pad a sentence into a fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "def pad(line, max_len, padding_token):\n",
    "    if len(line) > max_len:\n",
    "        return line[:max_len]\n",
    "    return line + [padding_token] * (max_len - len(line))\n",
    "\n",
    "pad(src_vocab[source[0]], 10, src_vocab.pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert a list of sentences into an `(num_example, max_len)` index array. We also record the length of each sentence without the padding tokens, called valid length. In addition, we add the special “&lt;bos&gt;” and “&lt;eos&gt;” tokens to the target sentences so that our model will know the signals for starting and ending predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "def build_array(lines, vocab, max_len, is_source):\n",
    "    lines = [vocab[line] for line in lines]\n",
    "    if not is_source:\n",
    "        lines = [[vocab.bos] + line + [vocab.eos] for line in lines]\n",
    "    array = nd.array([pad(line, max_len, vocab.pad) for line in lines])\n",
    "    valid_len = (array != vocab.pad).sum(axis=1)\n",
    "    return array, valid_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we construct data iterators to read data batches from the source and target index arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_nmt(batch_size, max_len):  # This function is saved in d2l.\n",
    "    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n",
    "    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)\n",
    "    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False)\n",
    "    train_data = gdata.ArrayDataset(\n",
    "        src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    train_iter = gdata.DataLoader(train_data, batch_size, shuffle=True)\n",
    "    return src_vocab, tgt_vocab, train_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [],
   "source": [
    "src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size=2, max_len=8)\n",
    "for X, X_valid_len, Y, Y_valid_len, in train_iter:\n",
    "    print('X =', X.astype('int32'), '\\nValid lengths for X =', X_valid_len,\n",
    "          '\\nY =', Y.astype('int32'), '\\nValid lengths for Y =', Y_valid_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}